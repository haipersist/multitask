多任务编程思想

序
  暂无
前言
第二章多进程，主要介绍进程的由来，进程是什么，何为多进程，多进程的原理、如何开发多进程以及知名架构对多进程的应用。
目录
第一章多任务处理的发展历史
1、高性能系统要求
     我们每天使用的微信、抖音、淘宝、京东等应用，每时每刻都有较高的访问量，全站QPS、TPS都达到一个很高的水平。因此这些系统对于高性能都具有较高的标准。如何在高流量的情况下，仍然能为用户能够快速、优质的体验是每一个系统开发人员都非常重视的一点。
     提高应用性能的手段有很多种，可以从不同的角度，包括缓存、横向扩展、纵向扩展、分布式等等。
   
 缓存
      缓存可以说是无处不在的，整个TCP/IP协议栈，任何一层都会涉及到缓存，计算机内部也存在各种缓存。比较常见的就是CPU的高速缓存，L1，L2，L3等等，高速缓存的存在是为了减少CPU的处理速度和IO之间的不匹配问题；还有在我们开发系统时会设置更多的缓存，包括客户端缓存、CDN缓存、进程内缓存、磁盘缓存、分布式缓存等等。通过缓存，既减少对应用的访问，又可以提高访问性能。当然，缓存也会存在一定潜在的风险点，最重要的就是缓存的一致性。如何能够保证当服务器对资源进行改动时，缓存的数据可以保证一致性。目前有很多种解决方案，比如Cache-Aside，感兴趣的可以观看《亿级流量架构》，本书对此不过多介绍。

扩展
除了缓存就是横向扩展。横向扩展简单来说，就是加机器，通常在电商大促的时候都会额外的加机器，加大量的机器。通过将整个系统的流量分散到不同的机器上，从而减少单个机器的负载。这种粗暴加机器是构建了我们服务的集群，集群上层可以通过不同的方式来实现负载均衡，通过http访问的可以使用nginx做负载均衡；通过RPC的，可以通过RPC框架来实现负载均衡，比如dubbo。当然，在网络层也可以实现负载均衡，比如LVS，nginx等等。

分布式
上面说的横向扩展是集群的部署模式，另外还有分布式的方式。分布式是一种革命，其一改之前的单体应用，将整个应用按照某种策略，比如DDD的思想，划分出多个小的微服务，不同的微服务构成了一个完整的应用系统。目前这种模式也是众多系统均采用的模式。比如电商系统，会划分出交易系统、商品系统、履约系统、客服系统、结算系统等等多个体量更小的系统，这些系统通过RPC或者HTTP的方式进行通信。分布式和集群是两种模式，但并不是互斥的，而是相辅相成的，整个系统是分布式的，但每个服务仍然会以集群的模式去部署。不过分布式在提升性能的同时，也会带来其他的问题，比如一致性。比较常见的就是在电商系统中，商品库存服务和订单服务，如何保证一致性，是一个很重要的问题。如果有深入了解的，可能会知道我们有TCC，消息表，TA啊等等处理方法。比较著名的就是阿里的Seata分布式事务。此外，分布式一个比较重要的概念就是CAP以及在实际应用中衍生出的BASEL理论等等。这里就不再赘述了。

纵向扩展
纵向扩展和横向扩展一样必不可少，机器规模不仅要大，每台机器的性能也同样重要。
纵向扩展简单说就是提升我们单台机器的处理性能。比如增加CPU核数，提高内存容量，使用SSD磁盘、提升带宽等等。

多任务处理
好，到了我们的重头戏，就是多任务处理。上面提到了纵向扩展，即硬件配置上来了，但是如果程序写得太垃圾，无法充分利用计算机资源，火车跑得快，全靠车头带，软件的能力至关重要。多任务处理的本质就是追求更高的处理速度，从而在整体上提高单台机器的处理性能。

2、多任务的发展历史
 单任务处理
  最早的计算机是一种单任务处理的逻辑。如果有了解过计算机历史的应该可以知道，第一代计算机虽然是一个庞然大物，但其最初设计的初衷就是做一些数据运算，本身的运算能力不是很强，每秒的运算能力也不强。当时想要计算机执行我们想要的某些任务，其基本流程是首先将指令打到卡片上，然后放到计算机里面执行，俗称穿孔打卡编程。卡片类似如下：

   看这图片，最早的还不是二进制，是10进制的。
这种运行方式，就是每次把任务放到计算机里面执行。虽然当时的计算机执行效率相对于现在不高，但是在当时相对于人的计算是非常可观的。当时也是为了美国进行弹道计算而产生的。
由于最早的计算机本身没有存储功能，因此在输入输出端会浪费大量的时间，计算机大部分时间是空闲的。所以整体下来，单任务处理还是太慢了，随着对计算机要求的提高，其已经完全不满足需求了。

  单道批量处理
   单任务处理效率过于低下，因此后来的计算机，即第二代计算机，出现了批量处理任务。
批量处理任务是将一个一个任务进行合并，然后一起放到计算机里进行执行。计算机拿到任务后，会按照顺序一个一个地执行。
   相对于第一种，批量处理的效率大大提高了，减少了输入输出端的操作。可是虽然这些任务是批量的，但是却一直是顺序执行的。就算前面的任务阻塞了，后面的任务也必须要等到前面的任务执行完毕才可以开始执行。因此，其整体性能还是有很大的瓶颈。
随着集成电路和操作系统的出现，迎来了第三代计算机，也迎来了多道程序处理。

  多道程序处理
    针对于串行批量任务处理任务的这种做法，
随着第三代计算机的兴起，计算机在硬件和软件上都有了革命性的变化。硬件上引入了集成电路，软件上引入了操作系统。在硬件上，计算机每秒的计算能力更强，在操作系统中，内核通过某种策略，保证不同的进程可以切换，从而最大程度利用计算机的资源。比如当某个程序遇到阻塞了，CPU完全可以将该程序挂起，并调度其他就绪的进程执行，当执行完之后，就会再执行其他程序。当然这其中涉及到CPU的调度程序、调度算法等等，还是比较复杂的。上面说的例子也只是非抢占式任务，其实还有抢占式任务，可能就涉及到时间片轮转、中断等等概念。
此外，如果计算机是多核CPU，在不同处理器上的程序，完全可以并行执行。这个核上面所讲到的并发还不是完全相同的概念。并行可以保证不同处理器同时执行不同的任务；而迸发强调的是在单个处理器上，任务在不停地切换，从而保证程序可以迸发的执行。本质上，处理器同时只能处理一个任务。
3、并发编程技术的介绍
 操作系统的并发
在《深入理解计算机系统》这本书中，对于迸发的定义是如果逻辑控制流在时间上重叠，则可称之为迸发。如果学过操作系统原理的课程，应该可以知道，操作系统会通过不同的策略来运行多个应用。
在应用级，作为开发者，可以开发出迸发的程序。目前主要包括多进程、多线程、IO多路复用以及协程，其中前三个都是操作系统提供的功能，最后一个协程是在应用级别来实现的，其并不是操作系统本身提供的构造。 
对于多任务技术，我们知道需要在不同任务间切换，那就涉及到调度的问题以及当前任务现场保护的问题。
系统调度
 那当我们要从就绪队列中选择进程时是通过什么原则呢？这时候就需要CPU完成调度。
至于调度的时机，即该何时去选择，以及选择哪个进程去执行。这还要取决于当前系统是抢占式还是非抢占式系统。
非抢占式系统：当前进程会主动放弃CPU；
抢占式系统：
中断请求时，会将当前进程转成就绪状态。那通常情况下，如果时间片轮转，就会在时间片用完了，会被抢占。或者有一个进程从等待变就绪了，且其更急迫需要执行，就会抢占当前进程。
CPU的调度最重要的目标是能够提供最大的吞吐量以及CPU的利用率。
目前存在的几种CPU调度算法：
先来先服务算法；
短进程优先算法；
最高响应比优先算法；
时间片轮转算法；
多级反馈队列算法；
公平共享调度算法；

先来先服务算法FCFS：
在就绪队列中，先进入的进程会先被执行。看下面的队列：

上面的队列，P1执行时间24，P2执行时间3，到达时间24，P3执行时间3，到达时间24.
上面队列的平均周转时间（24+27+30)/3。
看上面的例子，P1是一个执行时间很长的进程，在这种算法下，他会是先执行的，这就导致整个队列的平均周转时间可能比较长，这取决于长进程排在哪里，也就是周平均等待时间波动比较大。
另外一个问题是IO资源和CPU资源的利用率比较低。假如现在有一个CPU密集型任务的长进程正在执行，此时IO资源是空闲的，但是呢这种算法下，IO资源也并没有利用上，即使后面有IO密集型任务，也是只能干等着。这是一种非抢占式的算法。
短进程优先算法：
他是对FCFS的一种改进，它会优先执行短进程的，就绪队列会按照时间来排序。从而缩短了平均周转时间。
但怎么选择是一个问题，每个进程预计执行时间是多少呢？它要么希望用户告知，要么会通过某种算法预估执行时间。
上面是一个最优排序的算法，可以保证平均等待时间最短。
此外，短进程优先有一种算法改进，即加入当前有个进程执行，这时如果又到来一个进程，它执行时间要比正在执行进程剩余时间还短，那么它可以进行抢占。
那它有一个缺点，就是就绪队列不断地进入进程，由于它会把短的排在前面，可能不断地有短进程进入，就导致长进程永远得不到执行，导致长进程饥饿。这是一个问题。
最高响应比优先算法：
基于上面说的饥饿的问题，又有一种改进算法叫最高相应比优先算法。
相应比计算公式 rate = (w+s)/s， w表示等待时间，s表示预估执行时间。等待时间越长，rate越来越大，这样就可以避免长进程长时间等待出现饥饿的问题。该算法不允许抢占。
时间片轮转算法：
时间片是分批处理资源的基本执行单元。时间片轮转是为每个进程分配相同时间片，然后通过时钟中断，完成进程切换。
时间片轮转算法最重要的是时间片大小的设置。
设置太大，可能就变成先进先服务了。设置太小，可能会引起频繁的进程上下文切换。基于经验，一般都是设置成进程切换的1%.
多级反馈队列算法
把就绪队列分成多个独立的子队列，每个队列可以有不同的优先级，时间片的大小根据队列的优先级设置不同值。比如CPU密集型地可以放在低优先级，IO密集型地可以放在高优先级队列。进程可以放到不同的队列中，比如某个进程在一个队列的时间片内没有执行完，会把该进程放到低优先级队列中。
公平共享调度算法FSS
该算法会控制用户对资源的访问，可能让更重要的用户访问资源。
多处理机调度
目前计算机都是多处理器，因此都是多CPU组成的多处理器系统。在多处理器系统中，通常都是每个处理器有自己的调度程序，然后在访问共享资源时会进行同步。目前存在的算法：
1、静态资源算法
一个进程从开始到结束都在一个固定处理器，每个处理器有自己的就绪队列。这种算法开销小，但可能导致调度不均衡。
2、动态进程分配
每个进程在运行时可分配到各个处理器上，所有CPU共享一个就绪队列。这种做法开销是比较大的，因为它每次都要选择到哪个处理器上，但它却可以实现负载均衡。

任务切换
除了系统调度之外，另外一个重要概念就是当前任务的现场保护。
就说线程上下文切换，CPU在处理多任务时，需要不断地完成不同的线程之间的调度，而每个线程都会有自己独立的上下文环境（程序计数器，寄存器，栈等等）。当某种因素引起了切换，就要执行相应过程、
通常引起线程的上下文切换的因素主要包括：
当前执行任务的时间片用完之后，系统CPU正常调度下一个任务；
当前执行任务碰到IO阻塞，调度器将此任务挂起，继续下一任务；
多个任务抢占锁资源，当前任务没有抢到锁资源，被调度器挂起，继续下一任务；
用户代码挂起当前任务，让出CPU时间；
硬件中断；

无论是哪种方式引起的切换，首先要做的就是现场保护，主要向寄存器，程序计数器等等内容。只是进程间的切换要比线程切换的代价更大。
在Linux中，可以用vmstat来统计上下文切换的次数。
vmstat主要是用来在给定时间间隔收集当前机器的一些性能指标，比如CPU的利用率，线程切换次数，内存使用情况等等。




当然上下文的切换也不仅仅是局限于进程或者线程间的切换。由于我们应用程序都是通过系统调用来访问硬件资源的，所以还会涉及到用户态到内核态，以及内核态到用户态的上下文切换，这里就不再细说了，感兴趣的可以看我写过的Linux零拷贝技术讲解。

多进程
多进程是最容易实现的一种并发编程。通过多个进程来同时完成一个任务，由内核来调度和执行。
比如有个C/S场景，多个客户端请求服务器获取相应的资源。如果是非多进程的，可能是如图所示：


此时，客户端2必须要等Server处理完客户端1的请求才能继续得到服务器的响应。如果在当我们服务端开了多个进程来服务客户端，整个系统的性能上就会有一定的提升。接着上个图，在Server端再fork出一个子进程，如图所示：


由于不同进程使用不同的虚拟地址空间，导致进程间需要通过显示的方式实现进程间通信。

多线程
 多线程，这应该是很多程序员看到最多的概念，因为任何一个JAVA程序员都会或多或少接触到。在JAVA中，多线程技术的应用得淋漓尽致。由于其是进程内的资源，多个线程可以共享同一个内存地址空间，导致其进行线程间切换所付出的代价更小。但也正因为共享同一个地址空间，也带来了很多安全性的问题以及性能问题。因此，多线程相对于多进程来说，其复杂度要更高。在JAVA中，你可以看到各种因为线程安全性设置的变量、锁以及同步对象。还有出于性能考虑而设计的各种线程池等等。在第三章节中会详细介绍。

IO多路复用
一个IO请求可能包括数据准备，数据拷贝和处理。通常情况，IO操作是很费时的。如果一个请求在准备阶段阻塞住了，应该再处理其他的请求，直到其数据准备好。
IO多路复用是在单个进程上下文完成的。内核一旦发现进程内的一个或者多个IO条件准备读取，它就通知该进程去处理。当然触发机制有很多种，比如select,poll,epoll等。在第四章节有详细介绍。
在这种并发编程中，应用程序在一个进程的上下文进行调度，所有的流共享同一套地址空间。
与多进程、多线程相比，不必做进程或线程的创建和切换，减少了很多不必要的开销。在实际应用中，IO多路复用的使用也比较广泛，诸如Redis,Netty以及Python3的异步IO也是基于IO多路复用实现的。可见，其性能上所具备的无可比拟的优势。当然，通常情况，其也要配合着多线程一起使用，比较经典的Reactor模型，在第四章节会详细讲述。

协程
最早接触协程的概念，是我在写Python2时，那时有yield,send等基于生成器的协程，还有gevent库。后来跳槽到了小米，接触了Golang，才算真正认识了协程。
这种并发技术并不是操作系统直接支持的，而是开发者在应用中实现的。当发生阻塞时，主动让出CPU，从而实现在不同的逻辑流进行切换。
目前协程在很多新兴势力中得到较大范围的应用，golang就是其中之一，协程是其唯一的并发编程技术，相对于其他并发技术，协程更加简洁、高效。在第五章会对协程有更详细的介绍。

总结
迸发编程带来性能提升的同时，也引出了其他问题，或者说我们需要重点关注并解决的问题，其中安全性是最关键的问题，对于全局或者共享变量可能要考虑其安全性。比如在多线程中，线程安全性是最重要的课题。当有多个线程同时修改同一个变量时，如何保证所得到的结果是可预见的、正确的就是线程安全性的问题了，这个后续会详细说明。
在迸发编程中，还有其他很多的问题。比如多进程可能要考虑的是进程间通信、死锁等等。


































第二章 多进程
1、进程介绍
 1.1 进程定义	
在介绍多进程之前，首先是要了解什么是进程，主要是用来做什么的？它是由什么组成的？
进程是操作系统中的一个重要概念，当某一个应用程序被加载到内存中运行了，那么就可以称之为进程。注意这里的重点一是应用程序，二是运行中。对于一个应用程序，只有在操作系统中运行了，才可以称为进程，进程是操作系统赋予程序的生命。
在Linux种，可以用ps或者top 来查看当前系统中的进程。例如：

其中pid表示的就是进程id（Process Id)。操作系统会为每一个进程分配一个唯一的id。

在一台计算机内，可以运行多个不同的进程，而进程与进程之间是完全相互隔离的，互不干扰。而这要得益于操作系统的资源分配机制。操作系统为每一个进程都会分配一个独立的虚拟地址空间，其中包括内核空间和用户空间。也就是说，进程是系统资源分配的最小单元，每一个进程都有一个完整的虚拟地址空间。下面就介绍一下虚拟地址空间。

1.2 虚拟地址空间
   
提到虚拟地址空间，不得不说其产生的背景。在早期的计算机中，程序要运行是直接加载到物理内存中的，它会提前分配绝对物理地址，然后加载执行，比如我们早期使用的手机，像小灵通等等。这种运行方式带来的问题有很多。一是当时的物理内存都比较小，基本上运行几个程序，基本上内存就满了；二是物理地址不隔离，这就导致非常容易出现程序被某些恶意的程序篡改；三是由于物理地址是提前分配一段连续的地址，所以非常可能出现碎片，即某些空闲的地址，这就造成了内存使用效率非常低，无法充分利用内存。本来内存就很小，还有碎片，实在是无法忍受。
针对上述的问题，后续衍生除了覆盖和交换技术。
覆盖：将程序自身按照功能划分出多个模块，然后将不可能同时运行的模块共享一块内存空间；
交换：即将内存和外存交换，将暂时不用的内存数据移动到外存设备中；

虽然这两种技术在一定程度上解决了内存的使用问题，但是由于其本身存在天然的复杂性（覆盖技术需要开发者自己划分模块），此外也并不能解决地址隔离的问题。基于此，虚拟内存技术应运而生。这就是虚拟地址空间产生的背景。
虚拟存储就是同时利用内存和外存一起实现的一种方式。在进程执行时，会将暂时不需要的程序或数据放在内存外，在需要时通过缺页异常获取数据。具体在我写的操作系统内存管理有提到这个，可以参考我之前写的文章： https://hbnnforever.cn/article/osmemorysh.html。
虚拟地址空间是连续的，大小是固定的，对于32位操作系统，会分配2^32，即4GB的空间；对于64位操作系统，则会分配2^64。而一个虚拟地址空间由内核空间和用户空间组成，比例为1:3。最上面的内核空间是预留给操作系统中的代码和数据的，每一个进程都是相同的。底部的用户空间用来存放用户进程定义的代码和数据，主要包括：
1、栈
编译器栈来完成函数调用，其在程序执行期间会动态地扩展和收缩。当我们调用函数时，栈会增长，每当函数返回时，栈就会收缩。它是向下增长的，且是连续的。栈由操作系统控制，和开发者无关。这里的栈和JAVA栈不是一个，但JAVA栈会使用进程的栈。
2、堆
堆是在程序运行时是由malloc创建的，学过C语言的应该都用过。堆可以在运行中动态地扩展或收缩。它是向上增长的，且不是连续的。要注意，这里的堆和JAVA堆是不同的。JAVA堆最后会利用进程的堆，但进程的堆并不仅仅是JAVA堆，其中的直接内存其实使用的也是进程的堆。
JAVA
3、程序代码和数据
代码是从以固定地址开始的，然后是数据段，存储全局变量。代码和数据区是直接初始化的，大小固定。
下面是一张比较标准的虚拟地址空间示意图（来自深入理解计算机系统）：


通常将进程称作PCB，即进程控制块。操作系统在内核空间维护了一张表，叫进程表。进程表记录了当前操作系统的进程信息，包括进程ID，进程优先级，进程状态以及指向进程的指针等等。对于任何一个进程，它都会存在上述的完整的虚拟地址空间。通过虚拟地址空间，操作系统可以对进程更灵活、更友好的管理。


2、多进程原理
我们在运行程序时，会感觉正在独占操作系统资源和硬件。而实际上，在整个过程中，还有很多其他的进程和当前进程一样正在执行，只是系统会根据调度算法来对不同的进程调度来实现进程的并发和并行。并发就是指单个处理器下多个进程交替地执行；而并行是不同进程分别在不同的处理器上执行。而这也是多进程编程的基本原理。
本文只说并发。不同进程交替地执行，这其中就涉及到进程的运行、暂停、再运行等等。因此对于每一个进程，都会存在以下几个状态：就绪、运行、阻塞等等。

2.1 进程的状态
1、就绪
进程本身已经准备好了运行需要的所有资源，就等着操作系统调度CPU来处理了。就绪状态在两种情况会出现，一是刚创建好的进程；二是因为当前CPU执行时间到（比如时间片执行完了）。
2、运行
比较好理解，进程拿到了CPU资源，处理器正在执行该进程。
3、阻塞
当正在执行的进程运行期间，由于出现阻塞，比如IO阻塞需要等待数据到达，此时进程会主动向系统发出申请将自己由运行状态变为阻塞状态，处于阻塞状态的进程是不占用CPU资源的。

下面是比较常见的状态转换过程：

1）就绪态——运行态：对就绪状态的进程，当系统为其分配了CPU来处理，该进程会从就绪状态变为运行状态；
2）运行态——阻塞态：正在运行的进程由于发生某等待事件而无法执行，则进程由运行状态变为阻塞状态，最常见的是IO阻塞，比如网络IO、磁盘IO等等；
3）阻塞态——就绪态：处于阻塞状态的进程，如果等待事件已经完成，比如数据已经准备i完成，处于阻塞状态的进程并不会立刻转入运行状态，而是先转入就绪状态，因为此时可能还没有CPU资源来处理，需要等待系统为其分配CPU来处理；
4）运行态——就绪态：正在运行的进程，由于为其调度的资源要被回收了，该进程便由执行状态转变为就绪状态。

进程上下文切换
从上面的进程状态的变化可以看到进程可能会由于分配的CPU资源用完了或者出现了IO阻塞等原因导致执行的进程由当前CPU处理的进程切换成另外的进程。进程间的切换也叫做进程的上下文切换。
在具体说进程上下文切换之前，要搞懂啥是上下文切换。计算机系统中发生上下文切换的场景非常多，比如系统调用上下文切换、中断上下文切换，进程上下文切换，线程上下文切换等等。简单来说，上下文切换即保存当前执行所需的上下文，然后切换到新的上下文。我们先了解一下系统调用引起的上下文切换。
上面已经提到，一个进程包括用户空间和内核空间。处于内核空间时可以访问任何硬件资源；而处于用户空间的进程访问资源是受限的。所以程序执行时，经常会发生系统调用，比如我们进行磁盘文件的读写，网络数据的读写等等。而在发生系统调用的过程中，由于需要从用户态切换到内核态，执行之后还需要从内核态切换回用户态，因此需要经历两次的上下文切换。上下文切换主要会做如下操作:
1、保存 CPU 寄存器里原来用户态的指令位
	2、为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。
	3、跳转到内核态运行内核任务。
	4、当系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。

   说完系统调用，再回到我们的进程上下文切换。进程的上下文切换要比系统调用复杂得多，因为系统调用是在进程内完成的。而进程的上下文切换需要涉及到两个进程。切换是由操作系统内核完成的，即需要从进程的内核态切换到另外一个进程的内核态。因此CPU在完成切换时，不仅要保存当前程序计数器和寄存器等内容，还要保存用户空间资源。待切换到新的进程后，还要刷新当前的虚拟内存。因此，进程的上下文切换是成本比较高的一种切换场景。
  下面这张示意图大致展示了进程切换时所做的工作。


进程间通信（IPC）
由于进程具备独立的虚拟地址空间，因此不同进程的资源是无法做到共享的。如果想实现进程间的通信，必须要借助某种渠道。目前比较通用的进程间通信方式有：消息队列、信号、信号量、管道、命名管道、共享内存等等。

消息队列
     消息队列，这个在平时并不少见。我们经常会用Redis,RocketMQ,Kafka等作为消息队列。操作系统中的消息队列大体和我们所见的比较相似，但其最大的特点是消息队列是存储在内核中的一种消息链表，是操作系统自身提供的一种功能。
不同的进程可以向消息队列中写入数据，接收进程异步地接收消息，其并不会因为消息为空出现阻塞。在linux中可以通过msgget创建消息队列，并通过msgsend,msgrecv发送和接收消息。

管道/命名管道
管道相对于命名管道也叫匿名管道。如果接触过零拷贝，会经常看到管道的技术。

匿名管道（父子进程）： 
grep -rl "xiaomi" | wc -l 

命名管道： 
mkfifo namedpipe 
echo "xiaomiyoupin" > namedpipe 

另一个进程： 
cat namedpipe

信号
在Linux中，信号随处可见，比如我们常见的kill进程 .kill -9,，几乎大家都会用到。其他常见的信号主要包括如下：
（1）SIGHUP：用户终端退出时，同一Session下的进程都会收到该信号，并终止。
	（2）SIGINT：程序终止信号。程序运行过程中，按Ctrl+C键将产生该信号，也就是改信号只能关闭前台进程。
	（3）SIGQUIT：程序退出信号。程序运行过程中，按Ctrl+\\键将产生该信号。
	（4）SIGBUS和SIGSEGV：进程访问非法地址。
	（5）SIGFPE：运算中出现致命错误，如除零操作、数据溢出等。
	（6）SIGKILL：用户终止进程执行信号。执行kill -9会发送该信号给目标进程。
	（7）SIGTERM：结束进程信号。shell下执行kill 进程pid发送该信号，相对于SIGKILL，该信号可以被进程捕获，因此收到该信号后，进程不会直接默认强制退出，而是可以执行一些处理程序，比如关闭文件等等。
	（8）SIGALRM：定时器信号。
	（9）SIGCLD：子进程退出信号。如果其父进程没有忽略该信号也没有处理该信号，则子进程退出后将形成僵尸进程。

信号量
Semaphore，这个学JAVA的肯定比较熟悉，信号量在JAVA中主要用来实现线程同步机制上。在操作系统中，其可以用来进行进程间通信，主要是控制资源的访问和进程同步。信号量控制了资源的访问数量。其是一个计数器，值为非负整数。当减到0时，证明没有资源了。如果进程要访问资源时，会将该信号量的值减去1，如果减到0了，就会进入到等待队列。专业术语上，有两个操作P，V，一个是减1，一个是+1。通过信号量可以实现进程间的同步，这个和线程间的信号量同步是一个概念。

共享内存
 这个就比较好理解了，类似于我们使用分布式缓存，找到一个公共内存，实现内存的共享，从而实现进程间的通信。比如mmap，可以将多个进程的虚拟地址空间的部分映射到同一块内存空间上。在这种情况下，其中一个进程改变了共享内存的内容，另外一个进程也会感知到。这个也被人们称作写时复制，Copy on Write。
既然多进程之间使用了共享内存，那就存在同时修改变量的问题，也就是同步和互斥的问题。同一时刻，只能有一个进程修改对应变量。要实现目的通常我们会加上锁，比如互斥所。但加锁非常容易引起一个常见的问题：死锁。在上学时考试以及应聘的笔试可能都会提到死锁，即死锁的产生条件。互斥、持有并等待、无抢占以及循环等待这四个条件。在开发中，要注意避免死锁的产生。

3、Linux多进程
在Linux中，所有进程构成了一个树形结构。树根是系统自动构造的，即在内核态下执行的0号进程，它是所有进程的祖先。
由0号进程创建1号进程（内核态），1号进程是开机时，内核调用的初始化进程-init。该进程负责准备操作环境，启动各种服务，执行rc.local等初始化文件。
ps ajxf    查看进程树
Ps命令是在日常工作中使用比较频繁的命令，通过该命令可以查看正在运行的进程，查看端口、查看父子进程等等。
如果查看TCP端口可以使用netstat命令。
PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
   
 1 28716 28716 28716 ?           -1 Ss       0  11:04 /srv/dailyblog/env/bin/python /srv/dailyblog/env/bin/supervisord -c supervisor.conf
28716 30684 30684 28716 ?           -1 S        0   4:43  \_ /srv/dailyblog/env/bin/python /srv/dailyblog/env/bin/celery -A dailyblog worker --loglevel=INFO
30684 30690 30684 28716 ?           -1 S        0   0:00  |   \_ /srv/dailyblog/env/bin/python /srv/dailyblog/env/bin/celery -A dailyblog worker --loglevel=INFO
28716 24372 24372 28716 ?           -1 S        0   0:07  \_ /srv/dailyblog/env/bin/python /srv/dailyblog/env/bin/gunicorn dailyblog.wsgi:application -c /srv/dailyblog/www/
24372 24377 24372 28716 ?           -1 Sl       0   0:14      \_ /srv/dailyblog/env/bin/python /srv/dailyblog/env/bin/gunicorn dailyblog.wsgi:application -c /srv/dailyblog/
24372 24380 24372 28716 ?           -1 Sl       0   0:18      \_ /srv/dailyblog/env/bin/python /srv/dailyblog/env/bin/gunicorn dailyblog.wsgi:application -c /srv/dailyblog/
24372 24383 24372 28716 ?           -1 Sl       0   0:19      \_ /srv/dailyblog/env/bin/python /srv/dailyblog/env/bin/gunicorn dailyblog.wsgi:application -c /srv/dailyblog/
24372 24384 24372 28716 ?           -1 Sl       0   0:25      \_ /srv/dailyblog/env/bin/python /srv/dailyblog/env/bin/gunicorn dailyblog.wsgi:application -c /srv/dailyblog/

除了0号和1号进程，其他进程都是通过fork创建的。即进程调用进程是先通过父进程以fork的方式复制一个与父进程相同的暂存进程，然后暂存进程开始以exec的方式加载实际要执行的程序。这也是我们进行多进程编程的方式。fork子进程之后，父进程会将当前虚拟地址空间的数据资源复制一份给子进程，但此时父进程和子进程使用的物理空间可能还是一份，只有当其中的父进程或者子进程进行了数据的修改，子进程才会拥有完全独立的数据。

下面是Linux下多进程编程的简单展示：
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

int main()
{
    printf("+++process %d start running!ppid = %d\n",getpid(),getppid());

    pid_t pid = fork();

    if(pid)//父进程
    {
        printf("parent:process %d start running!ppid = %d\n",getpid(),getppid());
        //do something
        //...
    }
    else//子进程
    {
        printf("child:process %d start running!ppid = %d\n",getpid(),getppid());
        //do something
        //...
        exit(0);
    }

    exit(0);
}


 
4、多进程实战
Php中，多进程是并发编程的唯一方式。当然后续也有其他框架引入了异步，swoole，其主要主也是借鉴了协程的思想，在php中也比较受到青睐，有人做过实验拿swoole在laravel中使用与springboot多线程做过实现，用ab模拟并发处理性能，swoole更胜一筹，当然我并没有实际验证过，感兴趣的可以去真正体会一下。本文首先展示在php中的多进程编程的简单示例。

public function addProcess($funcname,...$params) {
    $pid = pcntl_fork();
    switch ($pid) {
        case -1:
            print Color::red("子进程创建失败！\r\n");
            break;

        case 0:
            call_user_func($funcname,...$params);

            //任务执行完成，必须退出
            exit();
            break;

        default:
            print Color::yellow("It is in Main process\r\n");
            $this->childs[] = $pid;
    }
}

上面代码是我在项目引入的一段实现多进程的例子，多进程编程简单来说即调用fork函数创建子进程。如果pcntl_fork返回了0表示创建子进程成功了，并可执行对应的函数；如果返回-1就是失败了，否则就是还是在父进程中执行。
现在来看，实际上实现多进程还是比较简单的。主要是要注意两点。

1、避免僵尸进程
 	多进程编程最最重要的是避免僵尸进程。所谓的僵尸进程是指当子进程执行完成之后， 父进程并没有监听父进程的退出状态，从而导致子进程的相关信息并没有从进程表中删除。
那么产生过多的僵尸进程有什么危害呢？最大的危害就是资源泄露。由于僵尸进程仍然保存了进程表中的表象，这就可能导致大量僵尸进程仍然白白占用了大量的资源。这个其实和我们在JAVA编程中经常遇到内存泄露是大同小异，内存泄露最终的结果是OOM。那资源泄露，同样会严重整个服务器的性能。
那该如何避免僵尸进程的产生呢？
主要是多进程编程的时候，要记得对每一个子进程实现wait系统调用。在PHP中的示例如下：

while (count($this->childs)>0) {
   foreach ($this->childs as $key => $pid) 	
{  
   $res = pcntl_waitpid($pid,$status,WNOHANG);                if ($res == -1 || $res>0) {              
      unset($this->childs[$key]);   
     }
 }   
        }
 childs数组保存了父进程创建的所有子进程的进程号pid，对于每一个子进程，父进程都会调用waitpid来监听其状态，当子进程没有结束时，父进程调用waitpid会被阻塞，待子进程结束后，waitpid会立即返回。通过该方式，可以有效避免僵尸进程的产生。
 说完僵尸进程，要顺便说一下孤儿进程，孤儿进程的产生是因为子进程还在运行，父进程却已经退出了。此时，init进程会接过父进程的角色，会负责管理这些孤儿进程，init进程会循环调用waitpid。因此孤儿进程本质来讲对系统是没有任何危害的。

2、避免子进程的一些Socket连接复用。
   由于子进程会复制父进程的用户空间，自然也会复用在父进程的所有资源，包括Socket连接，如果此时直接使用Mysql或者Redis的一些TCP连接，这样就会导致因为时序的问题可能会如下的错误:
1)Packets out of order. Expected 1 received 0. Packet size=442221
2)MySQL server has gone away
这个也是我在前几年开发中遇到的问题，对这个印象还比较深刻。所以当我们要在新创建的子进程中读取网络数据时，记得要重新创建新连接。
$client = YpRedisClient::getDefaultRedisClient();        DB::connection()->reconnect();        
上面是在子进程中重新创立了Redis和Mysql的连接。
	
5、多进程案例讲解
一、Nginx
Nginx是标准的多进程模型，通过一个master来管理多个work子进程，子进程用于处理请求。


Nginx启动之后，首先会创建一个master进程，并创建需要监听的socket,这里就是我们创建socket的基本操作，bind对应的ip和端口，调用listen，随后master会fork一定数量的子进程，即worker，worker的数量是可以在配置文件中配置的，当然一般情况下为了避免进程上下文切换，基本上都是CPU核数。fork出的子进程会继承master资源，这其中也包括相同的socket，这时所有的worker复用了相同的ip和端口。
     此时的模型就是一个master，多个worker的多进程模型。其中master的作用就是为了接收外部的信号，监听worker的状态，负责worker的管理。Worker就是实际处理客户端请求的进程。
      
     






上面提到master要监听管理worker，这就涉及到进程间通信机制了，Nginx主要是采用三种方式来实现进程间通信：共享内存、信号和套接字。SocketPair是由数组组成的，fd[0]用于master写入，fd[1]用于worker进程读取。不过这里额外说一句，虽然Nginx提供了这种机制，但还是用的比较少，主要是通过信号和共享内存来完成IPC。
当外界来一个信号，比如nginx reload重新加载配置文件，master会重新加载配置，并启动新的worker，随后告知旧的worker关闭。

Nginx通过多进程+单线程（内部使用IO多路复用）来实现高并发处理，目前也是基本上取代了Apache，成为了应用最广泛的反向代理。在第四章节会对IO多路复用进行详细的介绍。
当然还要说的是惊群效应，这是多进程非常常见的一个问题。即多个进程会在同一时间被唤醒，然而实际上是不需要唤醒所有进程的。在Nginx中，是利用互斥锁来解决的,只有获取到互斥锁的worker才能够处理链接，而这个互斥锁是存储在共享内存中。
惊群效应无处不在，可能会在多进程，多线程等多种场景下存在，比如在ZK中，为了避免持有锁的线程释放锁后，多个线程同时争抢锁，通过临时节点序号来保证获取锁的先后顺序，从而避免了惊群效应。
二、Gunicorn
如果你是做Python开发的，对Gunicorn一定很熟悉。其是在Unix环境下使用的WSGI HTTP Server。WSGI全称是Python Web Server Gateway Interface，是一种接口规范，其本质是一种Web服务器和应用的通信协议。最早的时候是CGI，后来又衍生出FastCGI。WSGI是Python应用和Web服务器的通信协议，Gunicorn是一个具有高性能的Web服务器，类似于JAVA中的Tomcat。通常我们在部署Python应用时，都不会用框架自带的Server，比如Django，Flask，而这完全出于性能考虑。Gunicorn和Nginx一样是采用了多进程模型，pre-fork，master也同样负责worker管理，worker用来进行请求处理。不过和Nginx不同的是，worker采用了多线程或者协程的方式处理请求。


上面的stat中的l表示的是多线程。

在实际应用场景中，Nginx和Gunicorn会配合使用，各司其职，Nginx是反向代理的角色，可以做静态资源代理，可以实现负载均衡，而Gunicorn充当的就是Web Server的角色，解析Http请求，调用Python应用处理请求，返回请求。


接下来一起看下gunicorn的源码，看他是如何实现pre-fork的。

gunicorn的启动入口在工程目录下arbiter.py中的run方法，我保留了其中比较重要的部分。
  
1.def run(self): 
2.      #这里面就是启动了master进程。 
3.     self.start()  
4.   
5.    #管理进程
6.    self.manage_workers()  
7.  
8.    #随后无限循环，主要用来处理外界信号。
9.    while True:  
10.        self.maybe_promote_master()  
11.  
12.        sig = self.SIG_QUEUE.pop(0) if self.SIG_QUEUE else None  
13.        if sig is None:  
14.            self.sleep()  
15.            self.murder_workers()  
16.            self.manage_workers()  
17.            continue  
18.  
19.        signame = self.SIG_NAMES.get(sig)  
20.        handler = getattr(self, "handle_%s" % signame, None)  
21.      
22.        handler()  


在上面的代码中，manage_workers方法就是用来实际管理worker的，我们往下看：

1.def manage_workers(self):  
2.      #数量小于配置的num，就创建worker
3.       if len(self.WORKERS) < self.num_workers:  
4.           self.spawn_workers()  
5.  
6.       workers = self.WORKERS.items()  
7.       workers = sorted(workers, key=lambda w: w[1].age)  
8.       #数量大于配置的num，就kill掉相对老的
9.       while len(workers) > self.num_workers:  
10.           (pid, _) = workers.pop(0)  
11.           self.kill_worker(pid, signal.SIGTERM)  

可以看到，上面的主要目的就是控制当前的worker数量。Spawn_worker就是创建子进程用的。
1.def spawn_worker(self):  
2.      self.worker_age += 1  
3.      worker = self.worker_class(self.worker_age, self.pid, self.LISTENERS, 
4.                                 self.app, self.timeout / 2.0,  
5.                                 self.cfg, self.log)  
6.      #提前fork出子进程
7.      self.cfg.pre_fork(self, worker)  
8.      pid = os.fork()  
9.       #这里主要是做一些worker的初始化，具体做什么还要取决于用了哪种worker
10.      worker.init_process()  


6、总结
本章由浅入深，首先讲解了进程的概念，进程的作用，随后讲解了多进程的实现原理以及需要注意的事项，之后结合实际代码讲解了多进程的实战，包括Php,Python等等。最后就目前成熟的框架，讲解了其运行的原理以及如何实现高并发，如Nginx,Php-fpm以及Gunicorn等等。
通过本章的学习，希望大家可以掌握多进程的原理以及清楚如何去合理利用其完成并发编程，并能够认识到多进程的不足。针对进程的弊端，下一章会介绍多线程编程。
第三章 多线程
1、线程介绍
   在第二章中介绍了多进程，进程是资源分配的基本单元，但系统调度的最小单位是线程，也叫做轻量级进程。当我们运行一个应用程序时，真正执行代码逻辑的就是线程，可以叫做主线程。线程会有自己的线程ID，线程栈，程序计数器等。通常，线程主要包括内核线程和用户线程两种。内核线程是属于操作系统负责管理的，用户线程对于内核来说是无感知的。
通过主线程可以创建多个其他的线程，被称作对等线程。多个线程会在单个进程内执行，线程会使用同一个虚拟地址空间，即同样的资源。因为线程之间是可以实现资源共享的。
   和进程一样，线程同样具备不同的状态，在JAVA中，指定了6种状态，包括初始化，运行，阻塞，等待，超时等待以及终止。

2、多线程介绍
和多进程一样，多线程存在的目的也是提高资源利用率，但相比于多进程，多线程的资源消耗要更小，因为尽管也有上下文切换，但线程上下文切换所使用的资源要远远小于进程上下文切换。计算机在执行任务时，真正执行任务的是线程，就算是多进程，每个进程执行的任务仍然是线程，只是这时只有单线程。很多地方都会问到线程和进程的区别，一句话概括就是进程时资源分配的基本单元，线程是CPU调度的基本单元。
然而多线程同样存在一定的风险，上面已经提到过多线程可以共享进程内变量，这引出了一个问题，即线程安全性。在多个线程同时操作同一变量，如何保证其执行结果的可遇见性是具备一定挑战的。为了实现线程安全，有很多的实现机制，尤其是JAVA，实现了多种不同的线程同步机制，通信方式等。从volatile变量定义禁止重排序保证可预见性，到final的定义，再到synchronized原生锁，以及后续版本的演进升级，再到JAVA线程池和Executor，再到并发包种的多种Lock(ReentrantLock,读写锁等等），再到ForkJoin/Pool，Sephmare,CountDownLatch，还有就是Java的并容器.
包括ConcurrentHashMap,CopyOnWriteArrayList,阻塞队列等等。总之，JAVA在多线程方面所作的努力要远远高于其他语言。
学习多线程，一定是要学习JAVA多线程的，因为JAVA对多线程的使用非常得成熟，对于多线程的管理，线程池的使用，线程安全性，优先级等方面，JAVA都有比较深入的研究和成熟的成果。不过还有一点要明白，JAVA多线程也是最终创建操作系统的内核线程来实现的。
总结JAVA的使用，为了保证线程安全性，可以分为两大类，一个就是悲观互斥锁，通过内置的synchronized种的重量级锁，其他的基本上都是借助于volatile，CAS，AQS（同步队列）来实现的。
 
再说几个线程相关的现象。

 线程饥饿：线程饥饿是指由于某个线程长时间的占用，而不释放，导致其他线程永远得不到执行。通常只要通过设置线程的超时时间即可。 

 线程死锁：这个比较好理解了，和第二章介绍的多进程死锁是一个意思，即因为相互抢占资源，互相等待的场景。
 线程活锁：活锁和死锁是完全相反的。活锁的主要现象是不同线程相互推脱责任，谁也不执行，导致整体任务进行不下去。简单说就是相互谦让。



 线程需要考虑的几大方面：
        如何提高效率；
        如何保证线程安全性；
        如何实现线程通信；
linux中在之前的版本中并不支持多线程，后来是引入了用户级的线程，通常称为轻量级进程，在2.6版本之后才真正支持内核级线程。

   JAVA多线程
     讨论多线程，一定是离不开JAVA的，JAVA中使用最多的就是多线程，而且诸如JAVA作者们也将多线程的优势发挥得淋漓尽致，围绕着提升性能和线程安全做了很多的努力。

3、多线程实战
4、多线程案例讲解
5、总结
第四章 IO多路复用
1、IO介绍
2、JAVA IO的应用
3、Linux IO多路复用机制
4、IO多路复用实战
5、案例讲解
第五章 协程
1、协程原理介绍
2、Golang协程
3、协程案例讲解
第六章 总结



